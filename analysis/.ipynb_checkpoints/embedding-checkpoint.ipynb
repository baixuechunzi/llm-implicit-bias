{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8b59102-7c74-41b2-a504-56d2a9c00e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import weat\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d630f4-be85-4e2e-a65d-6f39d48a2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAI.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI()\n",
    "model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91bb062-3d94-4c1e-8488-093101986de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embeddings(words, version=0, word_type='target'):\n",
    "    # words: list of words like ['marvelous', 'superb']\n",
    "    # version: 0 is embedding just the word itself, 1 is embedding the word in a sentence, similar to CEAT\n",
    "    # word_type: target or attribute\n",
    "\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if version == 0:\n",
    "            to_embed = word\n",
    "        elif version == 1:\n",
    "            if word_type == 'target':\n",
    "                to_embed = \"Here is a list of words. For each word pick a word — \" + word # our prompt\n",
    "            elif word_type == 'attribute':\n",
    "                to_embed = \" — and write it after the word. The words are \" + word  # our prompt\n",
    "        if 'embedding' in model:\n",
    "            embedding = client.embeddings.create(input = [to_embed], model=model).data[0].embedding\n",
    "        elif 'llama' in model:\n",
    "            # https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d\n",
    "            # https://python.langchain.com/docs/integrations/text_embedding/llamacpp\n",
    "            llama = LlamaCppEmbeddings(model_path=\"/path/to/model/model.bin\")\n",
    "            embedding = llama.embed_query(to_embed)\n",
    "\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a62b2f3-0ee9-41a1-aa5a-075eb5e8ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and drop empty rows\n",
    "stimuli_df = pd.read_csv('iat_stimuli.csv')\n",
    "chained_df = pd.read_csv('result_chained.csv', index_col=0)\n",
    "iat_texts = chained_df['iat']\n",
    "\n",
    "# obtain stimuli from each gpt4 prompt\n",
    "attributes = []\n",
    "targets = []\n",
    "for txt in iat_texts:\n",
    "    words_before_hyphen = []\n",
    "    words_after_hyphen = []\n",
    "    \n",
    "    lines = str(txt).strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        cleaned_line = line.strip().lstrip('-').strip()\n",
    "        if '-' in cleaned_line:\n",
    "            before, after = cleaned_line.split('-', 1)  # split at the first hyphen only\n",
    "            words_before_hyphen.append(before.strip())\n",
    "            words_after_hyphen.append(after.strip())\n",
    "\n",
    "    attributes.append(words_before_hyphen) # a list of attribute words\n",
    "    targets.append(list(set(words_after_hyphen))) # only 2 target words\n",
    "\n",
    "# map random-order stimuli back to default, stigma, pos, neg.\n",
    "X = []\n",
    "Y = []\n",
    "A = []\n",
    "B = []\n",
    "for d in stimuli_df['dataset'].unique().tolist():    \n",
    "    X.append(stimuli_df.loc[stimuli_df['dataset'] == d]['A'].dropna().str.lower().tolist())\n",
    "    Y.append(stimuli_df.loc[stimuli_df['dataset'] == d]['B'].dropna().str.lower().tolist())\n",
    "    C = stimuli_df.loc[stimuli_df['dataset'] == d]['C'].dropna().str.lower().tolist()\n",
    "    A.append(C[:len(C)//2])\n",
    "    B.append(C[len(C)//2:])\n",
    "\n",
    "def flatten_and_deduplicate(input_list):\n",
    "    flattened_list = []\n",
    "    for element in input_list:\n",
    "        if isinstance(element, list):\n",
    "            flattened_list.extend(element)\n",
    "        else:\n",
    "            flattened_list.append(element)\n",
    "    return list(set(flattened_list))\n",
    "\n",
    "original_X = flatten_and_deduplicate(X)\n",
    "original_Y = flatten_and_deduplicate(Y)\n",
    "original_A = flatten_and_deduplicate(A)\n",
    "original_B = flatten_and_deduplicate(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996281ab-aae8-4c48-9cb2-c294cdfaa91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "version = 1\n",
    "\n",
    "emb_X_list = []\n",
    "emb_Y_list = []\n",
    "emb_A_list = []\n",
    "emb_B_list = []\n",
    "\n",
    "for i, (target,attribute) in enumerate(zip(targets,attributes)):\n",
    "        \n",
    "    target_X = []\n",
    "    target_Y = []\n",
    "    attribute_A = []\n",
    "    attribute_B = []\n",
    "    \n",
    "    for t in target:\n",
    "        # some string are having upper cases\n",
    "        if t.lower() in [x.lower() for x in original_X]:\n",
    "            target_X.append(t)\n",
    "        elif t.lower() in [y.lower() for y in original_Y]:\n",
    "            target_Y.append(t)\n",
    "    \n",
    "    for a in attribute:\n",
    "        if a in original_A:\n",
    "            attribute_A.append(a)\n",
    "        elif a in original_B:\n",
    "            attribute_B.append(a)\n",
    "\n",
    "    if len(target_X)==0 or len(target_Y)==0:\n",
    "        emb_X_list.append([[1e2]*1536])\n",
    "        emb_Y_list.append([[1e2]*1536])\n",
    "        emb_A_list.append([[1e2]*1536]*len(attribute_A))\n",
    "        emb_B_list.append([[1e2]*1536]*len(attribute_B))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56a02ae1-f224-4eb1-9a72-afdf0e351c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "version = 1\n",
    "\n",
    "emb_X_list = []\n",
    "emb_Y_list = []\n",
    "emb_A_list = []\n",
    "emb_B_list = []\n",
    "\n",
    "for i, (target,attribute) in enumerate(zip(targets,attributes)):\n",
    "        \n",
    "    target_X = []\n",
    "    target_Y = []\n",
    "    attribute_A = []\n",
    "    attribute_B = []\n",
    "    \n",
    "    for t in target:\n",
    "        # some string are having upper cases\n",
    "        if t.lower() in [x.lower() for x in original_X]:\n",
    "            target_X.append(t)\n",
    "        elif t.lower() in [y.lower() for y in original_Y]:\n",
    "            target_Y.append(t)\n",
    "    \n",
    "    for a in attribute:\n",
    "        if a in original_A:\n",
    "            attribute_A.append(a)\n",
    "        elif a in original_B:\n",
    "            attribute_B.append(a)\n",
    "\n",
    "    if len(target_X)==0 or len(target_Y)==0:\n",
    "        emb_X_list.append([[1e2]*1536])\n",
    "        emb_Y_list.append([[1e2]*1536])\n",
    "        emb_A_list.append([[1e2]*1536]*len(attribute_A))\n",
    "        emb_B_list.append([[1e2]*1536]*len(attribute_B))\n",
    "        continue\n",
    "    \n",
    "    emb_X = words_to_embeddings(target_X, word_type='target', version=version)\n",
    "    emb_Y = words_to_embeddings(target_Y, word_type='target', version=version)\n",
    "    emb_A = words_to_embeddings(attribute_A, word_type='attribute', version=version)\n",
    "    emb_B = words_to_embeddings(attribute_B, word_type='attribute', version=version)\n",
    "\n",
    "    \n",
    "    emb_X_list.append(emb_X)\n",
    "    emb_Y_list.append(emb_Y)\n",
    "    emb_A_list.append(emb_A)\n",
    "    emb_B_list.append(emb_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8087f6b3-c29c-4b7e-8b2c-998955c7f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embedding output\n",
    "data = {\n",
    "    'emb_X_list': emb_X_list,\n",
    "    'emb_Y_list': emb_Y_list,\n",
    "    'emb_A_list': emb_A_list,\n",
    "    'emb_B_list': emb_B_list,\n",
    "}\n",
    "with open('result_embed.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf419f-482a-4be3-8e4e-e3e4c6f79edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
